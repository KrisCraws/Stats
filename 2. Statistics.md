# Statistics
<br>

***population*** 
- collection of all items of interest
- denoted N
- numbers we obtain when working with a population are 'parameters'

***sample***
- subset of the population
- denoted n
- numbers we obtain when working with a population are 'statistics'
- Needs
  <ul> 
      - randomness: random sample is collected when each member of the sample is chosen from the population strictly by chance <br>
      - representativeness: representative sample is a subset of the population that accurately reflects the members of the entire population <br>
  </ul>

### Types of Data
1. Categorical: categories or groups, yes/no questions  
2. Numerical:
    <ul>
        <a. discrete - finite (ex. number of children you want to have) <br>
        b. continuous 0 infinite (ex. human's weight)
    </ul>


***Pareto diagram*** - ordered by frequencies and then there is a line that totals the cumulative frequency. On the right axis it has the %

generally, statisticians prefer 5 to 20 intervals to work with. can use ((largest value - smallest value)/number of desired intervals)

***relative frequency*** is made up of percentages

### Measures of Central Tendency
mean, median, and mode
<br>

### Skewness
- Right skewness means that the tail goes off to the right
- zero skew or symmetrical

***variance*** - measures the dispersion of a set of data points around their mean
- gives results in squared units

***standard deviation*** - square root of the variance squared
- most common measure of variability for a single set

***coefficient of variation*** - standard deviation / mean
- relative standard deviation (bc its simply the standard deviation relative to the mean)
- there are no units. it is perfect for comparisons.

***correlation coefficient*** - Cov(x,y) / (Stdev(x) * Stdev(y))
- correlation coeff. of 1 means the entire variability of one variable is explained by the other
- correlation of 0 means they have nothing in common
- correlation of x and y is the same as correlation of y and x

<br>

  Correlation does not imply causation <br>
  ***A common practice is to disregard correlations below 0.2***

  # Inferential Statistics Fundamentals

  ***Discrete uniform distribution*** - all outcomes have an equal chance of occuring

  ***normal distribution*** - also called the Gaussian distribution and some people call it a bell curve <br> 
  - mean, median and mode are all equal <br>
  - has no skew
  - notation: {\displaystyle {\mathcal {N}}(\mu ,\sigma ^{2})}

Every distribution can be standardized. Standardization is the process of transforming the variable into one with a mean of 0 and a std dev of 1 <br>

We can also standardize a normal distribution -> the result is a standard normal distribution

# Central Limit Theorem

***def*** : no matter the underlying distribution, the sampling distribution approcimates a normal; usually in order to apply this we need at least 30 observations (n>30)

-CLT allows us to perform tests, solve problems and make inferences using the Normal distribution, even when the population is not normally distributed 

### Reasons to use the normal distribution
- They approximate a wide variety of random variables
- distributions of sample means with large neough sample sizes could be approximated as normal
- all computable statitistics are elegant
- decisions based on normal distribution insights have a good track record

# Standard Error
***def*** : the standard deviation of the distribution formed by the sample means

- standard error decreases when sample size increases

***CALC:*** <br>${\frac{s}{\sqrt{n} }}$

# Estimator vs. Estimate
- An ***estimator*** is a method or formula for estimating an unknown parameter.
- An ***estimate*** is the actual value obtained when the estimator is applied to a specific sample of data.

# Point Estimate

[Point estimate - reliability factor * standard error, point estimate + reliability factor * standard error]
<br>The point estimate is the midpoint of the interval

# Confidence Intervals

***def*** - is the range within which you expect the population parameter to be

- use this is when the population variance is known 

***Calculation:***
<br>
$\bar{x} \pm z_{\alpha/2} {\frac{\sigma}{\sqrt{n} }}$
- same alpha we had when we defined our confidence interval. ex 95% con int => 5% alpha
- z of alpha come from the standard normal distribution table. Ex. we want to find values for 95% confidence. therefore z of alp/2 is z of 0.025. thus we look at the table for 1-0.025 = 0.975. find it in the table and add up the numbers on the two axises
- commonly used term for z is the ***critical value***

#### Student's T Distribution

- looks much like a normal distribution but with fatter tails
- fatter tails allows for a higher dispersion of variables and there is more uncertainty
- ***calc:*** 
<br>$t_{n - 1}$ , $_\alpha$ = $\frac{\bar{x} - \mu}{s / \sqrt{n} }$
- there are degrees of freedom (n -1)
- there is a student's t  table
- after 30 degrees of freedom, it becomes almost like the z-statistic
- if there are 50 or more observations, we use the z-table instead
- ***if we do not know the population variance, we use the student's t-table***
- to find a confidence interval for a population with an unknown variance: <br> $\bar{x} \pm t_{n - 1,  \alpha/2} {\frac{s}{\sqrt{n} }}$
- two key differences from the formula when the pop var is known.
  1) z vs t statistic
  2) instead of pop std dev ($\sigma$), we have sample std dev (*s*)
- when we know the pop variance we get a narrower confidence interval

### Margin of Error
***def*** - the formulas for the confidence intervals

## Definitions
***confidence interval*** = $\bar{x} \pm ME$<br>
***ME*** = $ Reliability Factor * \frac{STD}{\sqrt{n}}$

# Confidence Intervals. Two means. Dependent Samples

- dependent:
<br><emsp> - before and after situation
<br><emsp> - cause and effect
- independent subcases:
<br><emsp> 1) population variance known
<br><emsp> 2) population variance unknown but assumed to be equal
<br><emsp> 3) population variance unknown but assumed to be different

Side Notes: 
- in biology, normality is so often observed that we assume that such variables are normally distributed
- 95% confidence interval is one of the most used ones
- variance is std dev squared

### 1) CI for the difference of two means, independent samples, variance known

<br> $\sigma^2_{diff} = \frac{\sigma^2_e}{n_e} + \frac{\sigma^2_m}{n_m}$
<br>
<br>calculation for confidence interval: <br>
$(\bar{x} - \bar{y}) \pm z_{\alpha/2} \sqrt{ \frac{\sigma^2_x}{n_x} + \frac{\sigma^2_y}{n_y} }$

**Notes**:

- $(\bar{x} - \bar{y})$ is the difference point estimator
- $z_{\alpha/2}$ is the test statistic
- $\sqrt{ \frac{\sigma^2_x}{n_x} + \frac{\sigma^2_y}{n_y} }$ is the standard error

### 2) CI for the difference of two means, independent samples, variances unknown but assumed to be equal

Pooled variance formula:
<br> $s^2_p = \frac{(n_x - 1)s^2_x + (n_y - 1)s^2_y}{n_x + n_y -2} $
<br> where $s_x$ and $s_y$ are the std devs because $s_x^2$ and $s^2_y$ are the variances 

- bc it is an unknown variance, use the t statistic for CI
<br> $(\bar{x} - \bar{y}) \pm t_{n_x + n_y - 2, \alpha/2} \sqrt{\frac{s^2_p}{n_x} + \frac{s^2_p}{n_y}}$
- the degrees of freedom are equal to the total sample size minue the number of variables <br>
$n_x + n_y -2$

### 3) CI for the difference of two means, independent samples, variances unknown but assumed to be different
<br> $(\bar{x} - \bar{y}) \pm t_{v, \alpha/2} \sqrt{\frac{s^2_x}{n_x} + \frac{s^2_y}{n_y}}$
<br>
to estimate the degrees of freedom:
$v = \frac{(\frac{s^2_x}{n_x} + \frac{s^2_y}{n_y})^2}{(\frac{s^2_x}{n_x})^2 / (n_x -1) + (\frac{s^2_y}{n_y})^2 / (n_y -1) }$
